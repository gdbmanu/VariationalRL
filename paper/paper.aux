\relax 
\babel@aux{nil}{}
\@writefile{toc}{\contentsline {title}{End-Effect Exploration Drive for Effective Motor Learning}{1}\protected@file@percent }
\@writefile{toc}{\authcount {1}}
\@writefile{toc}{\contentsline {author}{Emmanuel Dauc\IeC {\'e}}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}\protected@file@percent }
\citation{kingma2013auto}
\citation{mohamed2015variational}
\citation{schmidhuber1991curious}
\citation{sutton2018reinforcement}
\@writefile{toc}{\contentsline {section}{\numberline {2}Method}{2}\protected@file@percent }
\citation{sutton2018reinforcement}
\newlabel{eq:effect-model}{{1}{4}}
\newlabel{eq:inv-policy}{{2}{4}}
\newlabel{eq:bayesian-update}{{4}{4}}
\newlabel{eq:TD-err}{{8}{5}}
\newlabel{eq:Q-update}{{9}{5}}
\newlabel{eq:Q-RL}{{10}{5}}
\citation{levine2013guided}
\citation{toussaint2009robot,levine2013guided}
\citation{mnih2013playing}
\newlabel{eq:var-RL}{{14}{6}}
\newlabel{eq:var-RL-lambda}{{15}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{7}\protected@file@percent }
\newlabel{fig:grid1}{{3}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A simple two-rooms environment. Starting from the upper-left corner, the agent is asked to plan a full sequence made of 7 elementary actions $a_1,...,a_7$, each elementary action being in (E,S,W,N). The only read-out from the environment is the final state, and a reward, that is equal to 1 if the final state is the lower-right corner, and 0 otherwise.}}{8}\protected@file@percent }
\bibstyle{splncs04}
\bibdata{biblio}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces End-Effect Exploration Drive (E3D)}}{9}\protected@file@percent }
\newlabel{algo:E3D}{{1}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusions}{9}\protected@file@percent }
\bibcite{kingma2013auto}{1}
\newlabel{fig:explore}{{3}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Task 1 : no reward is provided by the environment. Empirical distribution of the final states, after 5000 trials. {\bf  A.} Uniform policy. {\bf  B.} End-Effect Exploration Drive (E3D) algorithm. $\alpha =0.3$, $\beta = 1$, $\lambda =0.03$, $\eta =10^{-2}$.}}{10}\protected@file@percent }
\newlabel{fig:compare}{{3}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Task 2 : a reward $r=1$ is provided by the environment when the agent reaches the lower-right corner. Cumulative sum of rewards over 5000 trials, on 10 training sessions. The E3D algorithm is compared with state-of-the-art epsilon-greedy update. $\alpha =0.3$, $\beta = 100$, $\lambda =0.03$, $\eta =10^{-2}$, $\varepsilon =0.1$.}}{10}\protected@file@percent }
\bibcite{levine2013guided}{2}
\bibcite{mnih2013playing}{3}
\bibcite{mohamed2015variational}{4}
\bibcite{schmidhuber1991curious}{5}
\bibcite{sutton2018reinforcement}{6}
\bibcite{toussaint2009robot}{7}
