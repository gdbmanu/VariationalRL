\begin{thebibliography}{10}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }
\providecommand{\doi}[1]{https://doi.org/#1}

\bibitem{bays2007computational}
Bays, P.M., Wolpert, D.M.: Computational principles of sensorimotor control
  that minimize uncertainty and variability. The Journal of physiology
  \textbf{578}(2),  387--396 (2007)

\bibitem{bellemare2016unifying}
Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., Munos,
  R.: Unifying count-based exploration and intrinsic motivation. In: Advances
  in neural information processing systems. pp. 1471--1479 (2016)

\bibitem{fox2015taming}
Fox, R., Pakman, A., Tishby, N.: Taming the noise in reinforcement learning via
  soft updates. arXiv preprint arXiv:1512.08562  (2015)

\bibitem{friston2010free}
Friston, K.: The free-energy principle: a unified brain theory? Nature reviews
  neuroscience  \textbf{11}(2),  127--138 (2010)

\bibitem{haarnoja2017reinforcement}
Haarnoja, T., Tang, H., Abbeel, P., Levine, S.: Reinforcement learning with
  deep energy-based policies. arXiv preprint arXiv:1702.08165  (2017)

\bibitem{jordan1992forward}
Jordan, M.I., Rumelhart, D.E.: Forward models: Supervised learning with a
  distal teacher. Cognitive science  \textbf{16}(3),  307--354 (1992)

\bibitem{kaelbling1993learning}
Kaelbling, L.P.: Learning to achieve goals. In: IJCAI. pp. 1094--1099. Citeseer
  (1993)

\bibitem{kingma2013auto}
Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv preprint
  arXiv:1312.6114  (2013)

\bibitem{kurutach2018model}
Kurutach, T., Clavera, I., Duan, Y., Tamar, A., Abbeel, P.: Model-ensemble
  trust-region policy optimization. arXiv preprint arXiv:1802.10592  (2018)

\bibitem{lee2019efficient}
Lee, L., Eysenbach, B., Parisotto, E., Xing, E., Levine, S., Salakhutdinov, R.:
  Efficient exploration via state marginal matching. arXiv preprint
  arXiv:1906.05274  (2019)

\bibitem{levine2013guided}
Levine, S., Koltun, V.: Guided policy search. In: International Conference on
  Machine Learning. pp.~1--9 (2013)

\bibitem{miall1996forward}
Miall, R.C., Wolpert, D.M.: Forward models for physiological motor control.
  Neural networks  \textbf{9}(8),  1265--1279 (1996)

\bibitem{mishra2017prediction}
Mishra, N., Abbeel, P., Mordatch, I.: Prediction and control with temporal
  segment models. arXiv preprint arXiv:1703.04070  (2017)

\bibitem{mnih2013playing}
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
  D., Riedmiller, M.: Playing atari with deep reinforcement learning. arXiv
  preprint arXiv:1312.5602  (2013)

\bibitem{mohamed2015variational}
Mohamed, S., Rezende, D.J.: Variational information maximisation for
  intrinsically motivated reinforcement learning. In: Advances in neural
  information processing systems. pp. 2125--2133 (2015)

\bibitem{schmidhuber1991curious}
Schmidhuber, J.: Curious model-building control systems. In: Proc.
  international joint conference on neural networks. pp. 1458--1463 (1991)

\bibitem{sutton2018reinforcement}
Sutton, R.S., Barto, A.G.: Reinforcement learning: An introduction. MIT press
  (2018)

\bibitem{toussaint2009robot}
Toussaint, M.: Robot trajectory optimization using approximate inference. In:
  Proceedings of the 26th annual international conference on machine learning.
  pp. 1049--1056 (2009)

\end{thebibliography}
